# 第一周周报
对NLP的一些基本模型进行了复习，学习了pytorch，阅读了知识图谱的三篇论文，但是对于为什么用这样的网络结构还是不太清晰。
# 第二周周报
大致了解了NLP模型的发展史，明白了RNN面临的无法并行计算的问题，了解了Attention机制，了解了transform（[The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)）和bert，配置好了实验平台。
# 第三周周报
阅读论文 [Attention is All You Need](https://arxiv.org/abs/1706.03762)并动手实现了一遍，读了知识图谱的综述
# 第四周周报
看了老师发的综述，